{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb48f169",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "About the codebase, general flow, and what we modified\n",
    "\n",
    "![PPosseDiagram](./COMP9444-perceptron-posse.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f8b74",
   "metadata": {},
   "source": [
    "## Splitting Datasets and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2194ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda00547",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split_dataset.py\n",
    "'''\n",
    "\n",
    "# 350 : 150 images gives a 70 : 30 split, 400 : 100 gives a 80 : 20 split\n",
    "\n",
    "SPLIT_TRAIN = False\n",
    "\n",
    "for gesture in ['call', 'dislike', 'fist', 'like', 'mute', 'ok', 'peace', 'peace_inverted', 'stop', 'stop_inverted']:\n",
    "    if SPLIT_TRAIN == True:\n",
    "        imgnames = sorted(os.listdir(f\"./test_dataset/{gesture}\"))\n",
    "        for i in range(350):\n",
    "            shutil.copy(f'./test_dataset/{gesture}/{imgnames[i]}', f'./subsamples_train1/{gesture}/{imgnames[i]}')\n",
    "    else: # split test\n",
    "        imgnames = sorted(os.listdir(f\"./test_dataset/{gesture}\"), reverse=True)\n",
    "        for i in range(150):\n",
    "            shutil.copy(f'./test_dataset/{gesture}/{imgnames[i]}', f'./subsamples_test1/{gesture}/{imgnames[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "split_annotations.py\n",
    "'''\n",
    "\n",
    "for gesture in [\"call\", \"peace\", \"peace_inverted\", \"like\", \"dislike\", \"ok\", \"mute\", \"stop\", \n",
    "\"stop_inverted\", \"fist\"]:\n",
    "    print(f'current gesture: {gesture}')\n",
    "    f = open(f\"annotations1/{gesture}.json\", \"r\")\n",
    "\n",
    "    test_imgnames = os.listdir(f'subsamples_test1/{gesture}/')\n",
    "    train_imgnames = os.listdir(f'subsamples_train1/{gesture}/')\n",
    "\n",
    "    data = json.load(f)\n",
    "\n",
    "    out_test_f = open(f\"annotations_test1/{gesture}.json\", \"w\")\n",
    "    out_train_f = open(f\"annotations_train1/{gesture}.json\", \"w\")\n",
    "\n",
    "    out_test_dict = {}\n",
    "    for imgname in sorted(test_imgnames):\n",
    "        name = imgname[:-4]\n",
    "        out_test_dict[name] = data[name]\n",
    "\n",
    "    out_train_dict = {}\n",
    "    for imgname in sorted(train_imgnames):\n",
    "        name = imgname[:-4]\n",
    "        out_train_dict[name] = data[name]\n",
    "\n",
    "    json.dump(out_test_dict, out_test_f, indent=4)\n",
    "    json.dump(out_train_dict, out_train_f, indent=4)\n",
    "\n",
    "    f.close()\n",
    "    out_test_f.close()\n",
    "    out_train_f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dfdd6",
   "metadata": {},
   "source": [
    "## Dataset Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af90117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import torch.utils.data\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from typing import Dict, Tuple, List\n",
    "from omegaconf import DictConfig\n",
    "from preprocess import get_crop_from_bbox, Compose\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6c3ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset.py\n",
    "'''\n",
    "\n",
    "FORMATS = (\".jpeg\", \".jpg\", \".jp2\", \".png\", \".tiff\", \".jfif\", \".bmp\", \".webp\", \".heic\")\n",
    "\n",
    "\n",
    "class GestureDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for gesture classification pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            is_train: bool,\n",
    "            conf: DictConfig,\n",
    "            transform: Compose = None,\n",
    "            is_test: bool = False,\n",
    "            preprocess_option: str = None\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Custom Dataset for gesture classification pipeline\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        is_train : bool\n",
    "            True if collect train dataset else False\n",
    "        is_test: Bool\n",
    "            For metrics calculation on test set\n",
    "        conf : DictConfig\n",
    "            Config with training params\n",
    "        transform : Compose\n",
    "            Compose of transforms\n",
    "        preprocess_option: str\n",
    "            Option to normalize, grayscale or do nothing with the images - perceptron-posse\n",
    "        \"\"\"\n",
    "        self.conf = conf\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.preprocess_option = preprocess_option\n",
    "\n",
    "        self.labels = {label: num for (label, num) in\n",
    "                       zip(self.conf.dataset.targets, range(len(self.conf.dataset.targets)))}\n",
    "\n",
    "        self.leading_hand = {\"right\": 0, \"left\": 1}\n",
    "\n",
    "        subset = self.conf.dataset.get(\"subset\", None)\n",
    "\n",
    "        self.annotations = self.__read_annotations(subset)\n",
    "\n",
    "        users = self.annotations[\"user_id\"].unique()\n",
    "        users = sorted(users)\n",
    "        random.Random(self.conf.random_state).shuffle(users)\n",
    "\n",
    "        train_users = users[:int(len(users) * 0.8)]\n",
    "        val_users = users[int(len(users) * 0.8):]\n",
    "\n",
    "        self.annotations = self.annotations.copy()\n",
    "\n",
    "        if not is_test:\n",
    "            if is_train:\n",
    "                self.annotations = self.annotations[self.annotations[\"user_id\"].isin(train_users)]\n",
    "            else:\n",
    "                self.annotations = self.annotations[self.annotations[\"user_id\"].isin(val_users)]\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_files_from_dir(pth: str, extns: Tuple, subset: int = None) -> List:\n",
    "        \"\"\"\n",
    "        Get list of files from dir according to extensions(extns)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pth : str\n",
    "            Path ot dir\n",
    "        extns: Tuple\n",
    "            Set of file extensions\n",
    "        subset : int\n",
    "            Length of subset for each target\n",
    "        \"\"\"\n",
    "        if not os.path.exists(pth):\n",
    "            logging.warning(f\"Dataset directory doesn't exist {pth}\")\n",
    "            return []\n",
    "        files = [f for f in os.listdir(pth) if f.endswith(extns)]\n",
    "        if subset is not None:\n",
    "            files = files[:subset]\n",
    "        return files\n",
    "\n",
    "    def __read_annotations(self, subset: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read annotations json\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        subset : int\n",
    "            Length of subset for each target\n",
    "        \"\"\"\n",
    "        exists_images = []\n",
    "        annotations_all = pd.DataFrame()\n",
    "        path_to_json = os.path.expanduser(self.conf.dataset.annotations)\n",
    "        for target in self.conf.dataset.targets:\n",
    "            target_tsv = os.path.join(path_to_json, f\"{target}.json\")\n",
    "            if os.path.exists(target_tsv):\n",
    "                json_annotation = json.load(open(\n",
    "                    os.path.join(path_to_json, f\"{target}.json\")\n",
    "                ))\n",
    "\n",
    "                json_annotation = [dict(annotation, **{\"name\": f\"{name}.jpg\"}) for name, annotation in zip(\n",
    "                    json_annotation, json_annotation.values()\n",
    "                )]\n",
    "\n",
    "                annotation = pd.DataFrame(json_annotation)\n",
    "\n",
    "                annotation[\"target\"] = target\n",
    "                annotations_all = pd.concat([annotations_all, annotation], ignore_index=True)\n",
    "                exists_images.extend(\n",
    "                    self.__get_files_from_dir(os.path.join(self.conf.dataset.dataset, target),\n",
    "                                              FORMATS, subset))\n",
    "            else:\n",
    "                logging.info(f\"Databse for {target} not found\")\n",
    "\n",
    "        annotations_all[\"exists\"] = annotations_all[\"name\"].isin(exists_images)\n",
    "\n",
    "        return annotations_all[annotations_all[\"exists\"]]\n",
    "\n",
    "    def __prepare_image_target(\n",
    "            self,\n",
    "            target: str,\n",
    "            name: str,\n",
    "            bboxes: List,\n",
    "            labels: List,\n",
    "            leading_hand: str\n",
    "    ) -> Tuple[Image.Image, str, str]:\n",
    "        \"\"\"\n",
    "        Crop and padding image, prepare target\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : str\n",
    "            Class name\n",
    "        name : str\n",
    "            Name of image\n",
    "        bboxes : List\n",
    "            List of bounding boxes [xywh]\n",
    "        labels: List\n",
    "            List of labels\n",
    "        leading_hand : str\n",
    "            Leading hand class name\n",
    "        \"\"\"\n",
    "        image_pth = os.path.join(self.conf.dataset.dataset, target, name)\n",
    "\n",
    "        image = Image.open(image_pth).convert(\"RGB\")\n",
    "\n",
    "        width, height = image.size\n",
    "\n",
    "        choice = np.random.choice([\"gesture\", \"no_gesture\"], p=[0.7, 0.3])\n",
    "\n",
    "        bboxes_by_class = {}\n",
    "\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            x1, y1, w, h = bbox\n",
    "            bbox_abs = [x1 * width, y1 * height, (x1 + w) * width, (y1 + h) * height]\n",
    "            if labels[i] == \"no_gesture\":\n",
    "                bboxes_by_class[\"no_gesture\"] = (bbox_abs, labels[i])\n",
    "            else:\n",
    "                bboxes_by_class[\"gesture\"] = (bbox_abs, labels[i])\n",
    "\n",
    "        if choice not in bboxes_by_class:\n",
    "            choice = list(bboxes_by_class.keys())[0]\n",
    "\n",
    "        if self.is_train:\n",
    "            box_scale = np.random.uniform(low=1.0, high=2.0)\n",
    "        else:\n",
    "            box_scale = 1.0\n",
    "\n",
    "        image_cropped, bbox_orig = get_crop_from_bbox(image, bboxes_by_class[choice][0], box_scale=box_scale)\n",
    "\n",
    "        image_resized = ImageOps.pad(image_cropped, tuple(self.conf.dataset.image_size), color=(0, 0, 0))\n",
    "\n",
    "        gesture = bboxes_by_class[choice][1]\n",
    "\n",
    "        leading_hand_class = leading_hand\n",
    "        if gesture == \"no_gesture\":\n",
    "            leading_hand_class = \"right\" if leading_hand == \"left\" else \"left\"\n",
    "\n",
    "        return image_resized, gesture, leading_hand_class\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.annotations.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Image.Image, Dict]:\n",
    "        \"\"\"\n",
    "        Get item from annotations\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            Index of annotation item\n",
    "        \"\"\"\n",
    "        row = self.annotations.iloc[[index]].to_dict('records')[0]\n",
    "\n",
    "        image_resized, gesture, leading_hand = self.__prepare_image_target(\n",
    "            row[\"target\"],\n",
    "            row[\"name\"],\n",
    "            row[\"bboxes\"],\n",
    "            row[\"labels\"],\n",
    "            row[\"leading_hand\"]\n",
    "        )\n",
    "\n",
    "        label = {\"gesture\": self.labels[gesture],\n",
    "                 \"leading_hand\": self.leading_hand[leading_hand]}\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image_resized, label = self.transform(image_resized, label)\n",
    "\n",
    "        ####### added code to normalise and grayscale images - perceptron-posse #######\n",
    "        if self.preprocess_option == 'normalize':\n",
    "            mean, std = image_resized.mean([1, 2]), image_resized.std([1, 2])\n",
    "            transform_norm = transforms.Normalize(mean, std)\n",
    "            image_normalized = transform_norm(image_resized)\n",
    "            return image_normalized, label\n",
    "        \n",
    "        elif self.preprocess_option == 'grayscale':\n",
    "            transform_grayscale = transforms.Grayscale(num_output_channels=3)\n",
    "            image_grayscaled = transform_grayscale(image_resized)\n",
    "            return image_grayscaled, label\n",
    "        \n",
    "        elif self.preprocess_option == None:\n",
    "            return image_resized, label\n",
    "        \n",
    "        else:\n",
    "            raise RuntimeError(\"Incompatible argument for preprocess_option\")\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bea14a",
   "metadata": {},
   "source": [
    "## Cropping Image, Converting to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from torch import nn, Tensor\n",
    "from typing import Tuple, Dict, Optional, List\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "preprocess.py\n",
    "'''\n",
    "\n",
    "\n",
    "def get_crop_from_bbox(image: Image.Image, bbox: List, box_scale: float = 1.) -> Tuple[Image.Image, np.array]:\n",
    "    \"\"\"\n",
    "    Crop bounding box from image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : Image.Image\n",
    "        Source image for crop\n",
    "    bbox : List\n",
    "        Bounding box [xyxy]\n",
    "    box_scale: float\n",
    "        Scale for bounding box crop\n",
    "    \"\"\"\n",
    "    int_bbox = np.array(bbox).round().astype(np.int32)\n",
    "\n",
    "    x1 = int_bbox[0]\n",
    "    y1 = int_bbox[1]\n",
    "    x2 = int_bbox[2]\n",
    "    y2 = int_bbox[3]\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "    w = h = max(x2 - x1, y2 - y1)\n",
    "    x1 = max(0, cx - box_scale * w // 2)\n",
    "    y1 = max(0, cy - box_scale * h // 2)\n",
    "    x2 = cx + box_scale * w // 2\n",
    "    y2 = cy + box_scale * h // 2\n",
    "    x1, y1, x2, y2 = list(map(int, (x1, y1, x2, y2)))\n",
    "\n",
    "    crop_image = image.crop((x1, y1, x2, y2))\n",
    "    bbox_orig = np.array([x1, y1, x2, y2]).reshape(2, 2)\n",
    "\n",
    "    return crop_image, bbox_orig\n",
    "\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms: List[nn.Module]):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class ToTensor(nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "            image: Tensor,\n",
    "            target: Optional[Dict[str, Tensor]] = None\n",
    "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
    "        image = F.pil_to_tensor(image)\n",
    "        image = F.convert_image_dtype(image)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def get_transform() -> Compose:\n",
    "    transforms = [ToTensor()]\n",
    "    return Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc73f4c",
   "metadata": {},
   "source": [
    "## Custom Residual Neural Network Class Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c2e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Any, Callable, List, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41593c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "custom_resnet.py\n",
    "'''\n",
    "\n",
    "###################################################################################################\n",
    "# perceptron-posse\n",
    "\n",
    "class CustomBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block for ResNet\n",
    "    - Consists of two convolutional layers with 2-d batch normalization between followed by a dropout layer and \n",
    "    - ReLU activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                stride = 1,\n",
    "                identity_downsample: Optional[nn.Module] = None,\n",
    "                ):\n",
    "        super(CustomBlock, self).__init__()\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        # saving weights at beginning to reapply at end of block \n",
    "        identity = x\n",
    "\n",
    "        # main block architecture - 2 conv layers with batch norm and relu\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # dropout layer to help with overfitting - not necessary since our dataset is fairly small\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # if there is a size mismatch, downsample identity before readding\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        # reapply weights from beginning of block\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "#####################################\n",
    "\n",
    "class PerceptronPosseResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: CustomBlock,\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = len(layers)\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._custom_make_layers(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._custom_make_layers(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._custom_make_layers(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = None\n",
    "        self.avgpool = None\n",
    "        self.fc = None\n",
    "        if (self.num_layers == 4):\n",
    "            self.layer4 = self._custom_make_layers(block, 512, layers[3], stride=2)\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(512, num_classes)\n",
    "        elif (self.num_layers == 3):\n",
    "            self.layer4 = lambda x : x\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def _custom_make_layers(\n",
    "                    self, \n",
    "                    block,\n",
    "                    intermediate_channels,\n",
    "                    num_residual_blocks,\n",
    "                    stride):\n",
    "        \"\"\"Function to make resnet layers based on given parameters\"\"\"\n",
    "        layers = []\n",
    "\n",
    "        # used if size of identity is incompatible with output of block\n",
    "        identity_downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, intermediate_channels, kernel_size=1, stride=stride),\n",
    "            nn.BatchNorm2d(intermediate_channels)\n",
    "        )\n",
    "\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels, \n",
    "                intermediate_channels, \n",
    "                stride, \n",
    "                identity_downsample\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.in_channels = intermediate_channels # 256\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.in_channels, \n",
    "                    intermediate_channels\n",
    "                )\n",
    "            ) # 256 -> 64, 64*4 (256) again\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    block: CustomBlock,\n",
    "    layers: List[int],\n",
    "    **kwargs: Any,\n",
    ") -> PerceptronPosseResNet:\n",
    "    model = PerceptronPosseResNet(block, layers, **kwargs)\n",
    "    return model\n",
    "\n",
    "### duplicate and customise to make custom resnet classes - perceptron-posse ###\n",
    "\n",
    "def resnet18(**kwargs: Any) -> PerceptronPosseResNet:\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "    \"\"\"\n",
    "    return _resnet(CustomBlock, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "def resnet10(**kwargs: Any) -> PerceptronPosseResNet:\n",
    "    r\"\"\"ResNet-10 model\n",
    "    \"\"\"\n",
    "    return _resnet(CustomBlock, [1, 1, 1, 1], **kwargs)\n",
    "\n",
    "def resnet20(**kwargs: Any) -> PerceptronPosseResNet:\n",
    "    r\"\"\"ResNet-20 model\n",
    "    \"\"\"\n",
    "    return _resnet(CustomBlock, [3, 3, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c53fd",
   "metadata": {},
   "source": [
    "## Resnet Wrapper Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1352586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Dict\n",
    "from torch import nn, Tensor\n",
    "from .custom_resnet import resnet18, resnet10, resnet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ab20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "resnet.py\n",
    "'''\n",
    "\n",
    "# perceptron-posse\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom two headed ResNet configuration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes: int,\n",
    "            restype: str = \"ResNet18\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Custom two headed ResNet configuration\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            Number of classes for each task\n",
    "        freezed : bool\n",
    "            Freezing model parameters or not\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        pposse_model = None\n",
    "        if restype == 'ResNet18':\n",
    "            pposse_model = resnet18()\n",
    "        elif restype == 'ResNet10':\n",
    "            pposse_model = resnet10()\n",
    "        elif restype == 'ResNet20':\n",
    "            pposse_model = resnet20()\n",
    "\n",
    "        self.backbone = None\n",
    "        if pposse_model.num_layers == 4:\n",
    "            self.backbone = nn.Sequential(\n",
    "                pposse_model.conv1,\n",
    "                pposse_model.bn1,\n",
    "                pposse_model.relu,\n",
    "                pposse_model.maxpool,\n",
    "                pposse_model.layer1,\n",
    "                pposse_model.layer2,\n",
    "                pposse_model.layer3,\n",
    "                pposse_model.layer4,\n",
    "                pposse_model.avgpool\n",
    "            )\n",
    "        elif pposse_model.num_layers == 3:\n",
    "            self.backbone = nn.Sequential(\n",
    "                pposse_model.conv1,\n",
    "                pposse_model.bn1,\n",
    "                pposse_model.relu,\n",
    "                pposse_model.maxpool,\n",
    "                pposse_model.layer1,\n",
    "                pposse_model.layer2,\n",
    "                pposse_model.layer3,\n",
    "                pposse_model.avgpool\n",
    "            )\n",
    "\n",
    "        num_features = pposse_model.fc.in_features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_features, num_classes),\n",
    "        )\n",
    "\n",
    "        self.leading_hand = nn.Sequential(\n",
    "            nn.Linear(num_features, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, img: Tensor) -> Dict:\n",
    "        x = self.backbone(img)\n",
    "        x = torch.flatten(x, 1)\n",
    "        gesture = self.classifier(x)\n",
    "\n",
    "        leading_hand = self.leading_hand(x)\n",
    "\n",
    "        return {'gesture': gesture, 'leading_hand': leading_hand}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499bf4e",
   "metadata": {},
   "source": [
    "## General Helper Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac978d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from models.mobilenetv3 import MobileNetV3\n",
    "from models.resnet import ResNet\n",
    "from models.vit import Vit\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce83a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "utils.py\n",
    "'''\n",
    "\n",
    "def add_metrics_to_tensorboard(writer: SummaryWriter, metrics: Dict, epoch: int, mode: str, target: str) -> None:\n",
    "    \"\"\"\n",
    "    Add metrics to Tensorboard logs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    writer : SummaryWriter\n",
    "        Tensorboard log writer\n",
    "    metrics : Dict\n",
    "        Metrics value\n",
    "    epoch : int\n",
    "        Number of epoch\n",
    "    mode : str\n",
    "        Mode valid or train\n",
    "    target : str\n",
    "        Target name: gesture or leading_hand\n",
    "    \"\"\"\n",
    "    logging.info(f'{mode}: metrics for {target}')\n",
    "    logging.info(metrics)\n",
    "    for key, value in metrics.items():\n",
    "        writer.add_scalar(f'{key}_{target}/{mode}', value, epoch)\n",
    "\n",
    "\n",
    "def add_params_to_tensorboard(writer: SummaryWriter, params: Dict, epoch: int, obj: str, not_logging: Set) -> None:\n",
    "    \"\"\"\n",
    "    Add optimizer params to Tensorboard logs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    writer : SummaryWriter\n",
    "        Tensorboard log writer\n",
    "    params : Dict\n",
    "        Optimizer params for logging\n",
    "    epoch : int\n",
    "        Number of epoch\n",
    "    obj : str\n",
    "        Optimizer or learning scheduler for params logging\n",
    "    not_logging : List\n",
    "        Parameters that should not be logged\n",
    "    \"\"\"\n",
    "    for param, value in params.items():\n",
    "        if param not in not_logging:\n",
    "            writer.add_scalar(f'{obj}/{param}', value, epoch)\n",
    "\n",
    "\n",
    "def set_random_state(random_seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set random seed for torch, numpy, random\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random_seed: int\n",
    "        Random seed from config\n",
    "    \"\"\"\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "\n",
    "def save_checkpoint(\n",
    "        output_dir: str,\n",
    "        config_dict: Dict,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        epoch: int,\n",
    "        name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save checkpoint dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_dir : str\n",
    "        Path to directory model checkpoint\n",
    "    config_dict : Dict\n",
    "        Config dictionary\n",
    "    model : nn.Module\n",
    "        Model for checkpoint save\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer\n",
    "    epoch : int\n",
    "        Epoch number\n",
    "    name : str\n",
    "        Model name\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "\n",
    "    checkpoint_path = os.path.join(output_dir, f'{name}.pth')\n",
    "\n",
    "    checkpoint_dict = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'config': config_dict\n",
    "    }\n",
    "    torch.save(checkpoint_dict, checkpoint_path)\n",
    "\n",
    "# perceptron-posse\n",
    "\n",
    "def build_model(\n",
    "        model_name: str,\n",
    "        num_classes: int,\n",
    "        device: str,\n",
    "        checkpoint: str = None,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build modela and load checkpoint\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Model name e.g. ResNet18, MobileNetV3_small, Vitb32\n",
    "    num_classes : int\n",
    "        Num classes for each task\n",
    "    checkpoint : str\n",
    "        Path to model checkpoint\n",
    "    device : str\n",
    "        Cpu or CUDA device\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'ResNet18': ResNet(\n",
    "            num_classes=num_classes,\n",
    "            restype='ResNet18',\n",
    "        ),\n",
    "        'ResNet10': ResNet(\n",
    "            num_classes=num_classes,\n",
    "            restype='ResNet10',\n",
    "        ),\n",
    "        'ResNet20': ResNet(\n",
    "            num_classes=num_classes,\n",
    "            restype='ResNet20',\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    model = models[model_name]\n",
    "    print(f' ---------- Chosen Model: {model_name} ---------- ')\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        checkpoint = os.path.expanduser(checkpoint)\n",
    "        if os.path.exists(checkpoint):\n",
    "            checkpoint = torch.load(checkpoint, map_location=torch.device(device))[\"state_dict\"]\n",
    "            model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def collate_fn(batch: List) -> Tuple:\n",
    "    \"\"\"\n",
    "    Collate func for dataloader\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : List\n",
    "        Batch of data\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc284a5d",
   "metadata": {},
   "source": [
    "## Metric Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60abf323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "from omegaconf import DictConfig\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics.functional import accuracy, f1_score, precision, recall, auroc, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74027435",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "metrics.py\n",
    "'''\n",
    "\n",
    "def get_metrics(targets: Tensor, predicts: Tensor, conf: DictConfig, epoch: int, mode: str, writer: SummaryWriter,\n",
    "                target: str = \"gesture\") -> Dict:\n",
    "    \"\"\"\n",
    "    Calc metrics for predicted labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    targets : Tensor\n",
    "        Target class labels\n",
    "    predicts : Tensor\n",
    "        Predicted class labels\n",
    "    conf : DictConfig\n",
    "        Config\n",
    "    epoch : int\n",
    "        Number of epoch\n",
    "    mode : str\n",
    "        Mode valid or train\n",
    "    writer : SummaryWriter\n",
    "        Tensorboard log writer\n",
    "    target : str\n",
    "        Target name: gesture or leading_hand\n",
    "    \"\"\"\n",
    "    average = conf.metric_params[\"average\"]\n",
    "    metrics = conf.metric_params[\"metrics\"]\n",
    "    num_classes = conf.num_classes[target]\n",
    "    predicts_labels = torch.argmax(predicts, dim=1)\n",
    "    scores = {\n",
    "        \"accuracy\": accuracy(predicts_labels, targets, average=average, num_classes=num_classes),\n",
    "        \"f1_score\": f1_score(predicts_labels, targets, average=average, num_classes=num_classes),\n",
    "        \"precision\": precision(predicts_labels, targets, average=average, num_classes=num_classes),\n",
    "        \"recall\": recall(predicts_labels, targets, average=average, num_classes=num_classes)\n",
    "    }\n",
    "\n",
    "    if mode == \"test\":\n",
    "        scores[\"roc_auc\"] = auroc(predicts, targets, average=average, num_classes=num_classes)\n",
    "\n",
    "    needed_scores = {}\n",
    "    for metric in metrics:\n",
    "        needed_scores[metric] = round(float(scores[metric]), 6)\n",
    "\n",
    "    if mode == \"valid\" or mode == \"test\":\n",
    "        if target == \"leading_hand\":\n",
    "            class_names = [\"right\", \"left\"]\n",
    "        else:\n",
    "            class_names = conf.dataset.targets\n",
    "\n",
    "        cm = confusion_matrix(predicts, targets, num_classes)\n",
    "        df_cm = pd.DataFrame(cm, index=[i for i in class_names], columns=[i for i in class_names])\n",
    "\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        hm = sns.heatmap(df_cm, annot=True, fmt='.5g', cmap=\"YlGnBu\").get_figure()\n",
    "        writer.add_figure(f\"Confusion matrix for {target}\", hm, epoch)\n",
    "    return needed_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4f6ef",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import torch.utils\n",
    "import torch.optim\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from metrics import get_metrics\n",
    "from utils import collate_fn, add_metrics_to_tensorboard, add_params_to_tensorboard, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98a175",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "train.py\n",
    "'''\n",
    "\n",
    "class TrainClassifier:\n",
    "    \"\"\"\n",
    "    Gesture classification training pipeline:\n",
    "        -initialize dataloaders\n",
    "        for n epochs from training config:\n",
    "            -run one epoch\n",
    "            -eval on validation set\n",
    "            - metrics calculation\n",
    "            -save checkpoint\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def eval(\n",
    "            model: nn.Module,\n",
    "            conf: DictConfig,\n",
    "            epoch: int,\n",
    "            test_loader: torch.utils.data.DataLoader,\n",
    "            writer: SummaryWriter,\n",
    "            mode: str = \"valid\"\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Evaluation model on validation set and metrics calc\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            Model for eval\n",
    "        conf : DictConfig\n",
    "            Config with training params\n",
    "        epoch : int\n",
    "            Number of epoch\n",
    "        test_loader : torch.utils.data.DataLoader\n",
    "            Dataloader for sampling test data\n",
    "        writer : SummaryWriter\n",
    "            Tensorboard log writer\n",
    "        mode : str\n",
    "            Eval mode valid or test\n",
    "        \"\"\"\n",
    "        f1_score = None\n",
    "        if test_loader is not None:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                predicts, targets = defaultdict(list), defaultdict(list)\n",
    "                for i, (images, labels) in enumerate(test_loader):\n",
    "                    images = torch.stack(list(image.to(conf.device) for image in images))\n",
    "                    output = model(images)\n",
    "\n",
    "                    for target in list(labels)[0].keys():\n",
    "                        target_labels = [label[target] for label in labels]\n",
    "                        predicts[target] += list(output[target].detach().cpu().numpy())\n",
    "                        targets[target] += target_labels\n",
    "\n",
    "                for target in targets.keys():\n",
    "                    metrics = get_metrics(\n",
    "                        torch.tensor(targets[target]), torch.tensor(predicts[target]), conf, epoch, mode,\n",
    "                        writer=writer, target=target\n",
    "                    )\n",
    "                    if target == \"gesture\":\n",
    "                        f1_score = metrics[\"f1_score\"]\n",
    "                    add_metrics_to_tensorboard(writer, metrics, epoch, \"valid\", target=target)\n",
    "        return f1_score\n",
    "\n",
    "    @staticmethod\n",
    "    def run_epoch(\n",
    "            model: nn.Module,\n",
    "            epoch: int,\n",
    "            device: str,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            lr_scheduler_warmup: torch.optim.lr_scheduler.LinearLR,\n",
    "            train_loader: torch.utils.data.DataLoader,\n",
    "            writer: SummaryWriter\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run one training epoch with backprop\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            Model for eval\n",
    "        epoch : int\n",
    "            Number of epoch\n",
    "        device : str\n",
    "            CUDA or CPU device\n",
    "        optimizer : torch.optim.optimizer.Optimizer\n",
    "            Optimizer\n",
    "        lr_scheduler_warmup :\n",
    "            Linear learning rate scheduler\n",
    "        train_loader : torch.utils.data.DataLoader\n",
    "            Dataloader for sampling train data\n",
    "        writer : SummaryWriter\n",
    "            Tensorboard log writer\n",
    "        \"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        model.train()\n",
    "\n",
    "        lr_scheduler_params = lr_scheduler_warmup.state_dict()\n",
    "\n",
    "        if writer is not None:\n",
    "            optimizer_params = optimizer.param_groups[0]\n",
    "            add_params_to_tensorboard(writer, optimizer_params, epoch, \"optimizer\", {\"params\"})\n",
    "            not_logging = lr_scheduler_params.keys() - {\"start_factor\", \"end_factor\"}\n",
    "            add_params_to_tensorboard(writer, lr_scheduler_params, epoch, \"lr_scheduler\", not_logging)\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            step = i + len(train_loader) * epoch\n",
    "\n",
    "            images = torch.stack(list(image.to(device) for image in images))\n",
    "            output = model(images)\n",
    "            loss = []\n",
    "\n",
    "            for target in list(labels)[0].keys():\n",
    "\n",
    "                target_labels = [label[target] for label in labels]\n",
    "                target_labels = torch.as_tensor(target_labels).to(device)\n",
    "                loss.append(criterion(output[target], target_labels))\n",
    "\n",
    "            loss = sum(loss)\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            if not math.isfinite(loss_value):\n",
    "                logging.info(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                exit(1)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if lr_scheduler_warmup is not None:\n",
    "                lr_scheduler_warmup.step()\n",
    "\n",
    "            if writer is not None:\n",
    "                writer.add_scalar(f\"loss/train\", loss_value, step)\n",
    "                logging.info(f\"Step {step}: Loss = {loss_value}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def train(\n",
    "            model: nn.Module,\n",
    "            conf: DictConfig,\n",
    "            train_dataset: torch.utils.data.Dataset,\n",
    "            test_dataset: torch.utils.data.Dataset\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialization and running training pipeline\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            Model for eval\n",
    "        conf : DictConfig\n",
    "            Config with training params\n",
    "        train_dataset : torch.utils.data.Dataset\n",
    "            Custom train gesture classification dataset\n",
    "        test_dataset : torch.utils.data.Dataset\n",
    "            Custom test gesture classification dataset\n",
    "        \"\"\"\n",
    "\n",
    "        experimnt_pth = f\"experiments/{conf.experiment_name}\"\n",
    "        writer = SummaryWriter(log_dir=f\"{experimnt_pth}/logs\")\n",
    "        writer.add_text(f\"model/name\", conf.model.name)\n",
    "\n",
    "        epochs = conf.train_params.epochs\n",
    "\n",
    "        model = model.to(conf.device)\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=conf.train_params.train_batch_size,\n",
    "            num_workers=conf.train_params.num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            persistent_workers = True,\n",
    "            prefetch_factor=conf.train_params.prefetch_factor,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=conf.train_params.test_batch_size,\n",
    "            num_workers=conf.train_params.num_workers,\n",
    "            collate_fn=collate_fn,\n",
    "            persistent_workers = True,\n",
    "            prefetch_factor=conf.train_params.prefetch_factor,\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=conf.optimizer.lr,\n",
    "            momentum=conf.optimizer.momentum,\n",
    "            weight_decay=conf.optimizer.weight_decay\n",
    "        )\n",
    "\n",
    "        warmup_iters = min(1000, len(train_dataloader) - 1)\n",
    "        lr_scheduler_warmup = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=conf.scheduler.start_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "        best_metric = -1.0\n",
    "        conf_dictionary = OmegaConf.to_container(conf)\n",
    "        for epoch in range(conf.model.start_epoch, epochs):\n",
    "            logging.info(f\"Epoch: {epoch}\")\n",
    "            TrainClassifier.run_epoch(\n",
    "                model,\n",
    "                epoch,\n",
    "                conf.device,\n",
    "                optimizer,\n",
    "                lr_scheduler_warmup,\n",
    "                train_dataloader,\n",
    "                writer\n",
    "            )\n",
    "            current_metric_value = TrainClassifier.eval(model, conf, epoch, test_dataloader, writer)\n",
    "            save_checkpoint(experimnt_pth, conf_dictionary, model, optimizer, epoch, f\"model_{epoch}.pth\")\n",
    "\n",
    "            if current_metric_value > best_metric:\n",
    "                logging.info(f\"Save best model with metric: {current_metric_value}\")\n",
    "                save_checkpoint(experimnt_pth, conf_dictionary, model, optimizer, epoch, \"best_model\")\n",
    "                best_metric = current_metric_value\n",
    "\n",
    "        writer.flush()\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d58fd0",
   "metadata": {},
   "source": [
    "## Program Entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c5325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import torch.utils\n",
    "import torch.optim\n",
    "\n",
    "from dataset import GestureDataset\n",
    "from preprocess import get_transform\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from typing import Optional, Tuple\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from train import TrainClassifier\n",
    "from utils import set_random_state, build_model, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f0e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "run.py\n",
    "'''\n",
    "\n",
    "logging.basicConfig(format=\"[LINE:%(lineno)d] %(levelname)-8s [%(asctime)s]  %(message)s\", level=logging.INFO)\n",
    "\n",
    "\n",
    "def _initialize_model(conf: DictConfig):\n",
    "    set_random_state(conf.random_state)\n",
    "\n",
    "    num_classes = len(conf.dataset.targets)\n",
    "    conf.num_classes = {\"gesture\": num_classes, \"leading_hand\": 2}\n",
    "\n",
    "    model = build_model(\n",
    "        model_name=conf.model.name,\n",
    "        num_classes=num_classes,\n",
    "        checkpoint=conf.model.get(\"checkpoint\", None),\n",
    "        device=conf.device,\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _run_test(path_to_config: str):\n",
    "    \"\"\"\n",
    "    Run training pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_config : str\n",
    "        Path to config\n",
    "    \"\"\"\n",
    "    conf = OmegaConf.load(path_to_config)\n",
    "    model = _initialize_model(conf)\n",
    "\n",
    "    experimnt_pth = f\"experiments/{conf.experiment_name}\"\n",
    "    writer = SummaryWriter(log_dir=f\"{experimnt_pth}/logs\")\n",
    "    writer.add_text(f'model/name', conf.model.name)\n",
    "    \n",
    "    # perceptron-posse\n",
    "    \n",
    "    test_dataset = GestureDataset(is_train=False, conf=conf, transform=get_transform(), is_test=True, preprocess_option='grayscale')\n",
    "\n",
    "    logging.info(f\"Current device: {conf.device}\")\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=conf.train_params.test_batch_size,\n",
    "        num_workers=conf.train_params.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=conf.train_params.prefetch_factor,\n",
    "    )\n",
    "\n",
    "    TrainClassifier.eval(model, conf, 0, test_dataloader, writer, \"test\")\n",
    "\n",
    "\n",
    "def _run_train(path_to_config: str) -> None:\n",
    "    \"\"\"\n",
    "    Run training pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_to_config : str\n",
    "        Path to config\n",
    "    \"\"\"\n",
    "\n",
    "    conf = OmegaConf.load(path_to_config)\n",
    "    model = _initialize_model(conf)\n",
    "    \n",
    "    # perceptron-posse\n",
    "    \n",
    "    train_dataset = GestureDataset(is_train=True, conf=conf, transform=get_transform(), preprocess_option='grayscale')\n",
    "    test_dataset = GestureDataset(is_train=False, conf=conf, transform=get_transform(), preprocess_option='grayscale')\n",
    "\n",
    "    logging.info(f\"Current device: {conf.device}\")\n",
    "    TrainClassifier.train(model, conf, train_dataset, test_dataset)\n",
    "\n",
    "\n",
    "def parse_arguments(params: Optional[Tuple] = None) -> argparse.Namespace:\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Gesture classifier...\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-c\", \"--command\",\n",
    "        required=True,\n",
    "        type=str,\n",
    "        help=\"Training or test pipeline\",\n",
    "        choices=('train', 'test')\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--path_to_config\",\n",
    "        required=True,\n",
    "        type=str,\n",
    "        help=\"Path to config\"\n",
    "    )\n",
    "\n",
    "    known_args, _ = parser.parse_known_args(params)\n",
    "    return known_args\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_arguments()\n",
    "    if args.command == \"train\":\n",
    "        _run_train(args.path_to_config)\n",
    "    elif args.command == \"test\":\n",
    "        _run_test(args.path_to_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d4ab3",
   "metadata": {},
   "source": [
    "## Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_config.yaml\n",
    "\n",
    "dataset:\n",
    "  annotations: /home/souramandal/perceptron-posse-COMP9444/hagrid/annotations_test1/\n",
    "  dataset: /home/souramandal/perceptron-posse-COMP9444/hagrid/subsamples_test1/\n",
    "  targets:\n",
    "    - call\n",
    "    - dislike\n",
    "    - fist\n",
    "    - like\n",
    "    - mute\n",
    "    - ok\n",
    "    - peace\n",
    "    - stop\n",
    "    - stop_inverted\n",
    "    - peace_inverted\n",
    "    - no_gesture\n",
    "  image_size: [224, 224]\n",
    "  subset: 2000\n",
    "random_state: 42\n",
    "device: 'cuda'\n",
    "experiment_name: perceptron-posse-grayscaled-resnet20-testing\n",
    "model:\n",
    "  name: 'ResNet20'  # 'ResNet18' or 'ResNet10' or 'ResNet20'\n",
    "  pretrained: False\n",
    "  freezed: False\n",
    "  start_epoch: 0\n",
    "  checkpoint: /home/souramandal/perceptron-posse-COMP9444/hagrid/classifier/experiments/perceptron-posse-grayscaled-resnet20/best_model.pth # change checkpoint for testing run\n",
    "optimizer:\n",
    "  lr: 0.005\n",
    "  momentum: 0.9\n",
    "  weight_decay: 0.0005\n",
    "scheduler:\n",
    "  start_factor: 0.001\n",
    "train_params:\n",
    "  epochs: 75\n",
    "  num_workers: 12\n",
    "  train_batch_size: 64\n",
    "  test_batch_size: 20\n",
    "  prefetch_factor: 16\n",
    "metric_params:\n",
    "  metrics: ['accuracy', 'f1_score', 'precision', 'recall']\n",
    "  average: 'weighted'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
